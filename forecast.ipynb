{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from dataset import *\n",
    "import gc\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.gpu = 1\n",
    "        self.run_label = 0\n",
    "        self.feat_size = 128\n",
    "        self.batch_size = 8\n",
    "        self.q_learning_rate = 1e-2\n",
    "        self.n_sample = 1024\n",
    "        self.n_future = 10\n",
    "        self.n_past = 2\n",
    "        self.predictor_model = 'big'\n",
    "        self.log_root = '/data/hdim-forecast/log3'\n",
    "        \n",
    "args = Args()\n",
    "device = torch.device('cuda:%d' % args.gpu)\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run number = 1\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    args.name = 'pred=%s-seq=%d/%d-ns=%d-feat_size=%d-bs=%d-qlr=%.5f-run=%d' % \\\n",
    "        (args.predictor_model, args.n_past, args.n_future, args.n_sample, args.feat_size, \n",
    "         args.batch_size, args.q_learning_rate, args.run_label)\n",
    "    args.log_dir = os.path.join(self.log_root, args.name)\n",
    "    if not os.path.isdir(args.log_dir):\n",
    "        os.makedirs(args.log_dir)\n",
    "        break\n",
    "    args.run_label += 1\n",
    "print(\"Run number = %d\" % args.run_label)\n",
    "writer = SummaryWriter(args.log_dir)\n",
    "log_writer = open(os.path.join(args.log_dir, 'results.txt'), 'w')\n",
    "\n",
    "start_time = time.time()\n",
    "global_iteration = 0\n",
    "random.seed(args.run_label)  # Set a different random seed for different run labels\n",
    "torch.manual_seed(args.run_label)\n",
    "    \n",
    "def log_scalar(name, value, epoch):\n",
    "    writer.add_scalar(name, value, epoch)\n",
    "    log_writer.write('%f ' % value)\n",
    "    \n",
    "def message(epoch):\n",
    "    print(\"Finished epoch %d, time elapsed %.1f\" % (epoch, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureNetC(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=4608, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_model = FeatureNetC(args.feat_size)\n",
    "feat_model.load_state_dict(torch.load('pretrained/representation-c-%d.pt' % args.feat_size), strict=False)\n",
    "feat_model = feat_model.to(device)\n",
    "feat_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "multi_dataset = MovingMNISTMulti(train=True, n_past=args.n_past, n_future=args.n_future, n_sample=args.n_sample, deterministic=False, last_only=True)\n",
    "multi_loader = DataLoader(multi_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = predictors[args.predictor_model](args.feat_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.load_state_dict(torch.load('pretrained/predictor_128-10-big.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a set of query functions, each query function should be a map from [batch_size, 1, 64, 64] tensor into a real number\n",
    "intensity = [\n",
    "    lambda x: x[:, 0, :32, :].mean(dim=(1, 2)) - x[:, 0, 32:, :].mean(dim=(1, 2)),\n",
    "    lambda x: x[:, 0, :, :32].mean(dim=(1, 2)) - x[:, 0, :, 32:].mean(dim=(1, 2))\n",
    "]\n",
    "\n",
    "def get_func(left, right): \n",
    "    return lambda x: x[:, 0, left:left+32, right:right+32].mean(dim=(1, 2)) \n",
    "    \n",
    "has_digit = [get_func(left, right) for left in [0, 16, 32] for right in [0, 16, 32]]\n",
    "\n",
    "queries_all = intensity + has_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(device=device, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# [self.n_past, batch_size, 1, 64, 64]\n",
    "with torch.no_grad():\n",
    "    samples = sampler.simulate(bx[:, :2].to(device).permute(1, 0, 2, 3, 4), n_sample=100, n_future=10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i in range(10):\n",
    "    for j in range(12):\n",
    "        plt.subplot(10, 12, i*12+j+1)\n",
    "        plt.imshow(samples[i, j, 0, 0].cpu())\n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, time elapsed 3196.0\n",
      "Finished epoch 1, time elapsed 3310.9\n",
      "Finished epoch 2, time elapsed 3420.5\n",
      "Finished epoch 3, time elapsed 3533.8\n",
      "Finished epoch 4, time elapsed 3650.0\n",
      "Finished epoch 5, time elapsed 3765.5\n",
      "Finished epoch 6, time elapsed 3879.6\n",
      "Finished epoch 7, time elapsed 3996.6\n",
      "Finished epoch 8, time elapsed 4113.9\n",
      "Finished epoch 9, time elapsed 4232.4\n",
      "Finished epoch 10, time elapsed 4347.5\n",
      "Finished epoch 11, time elapsed 4465.1\n",
      "Finished epoch 12, time elapsed 4575.5\n",
      "Finished epoch 13, time elapsed 4695.1\n",
      "Finished epoch 14, time elapsed 4808.6\n",
      "Finished epoch 15, time elapsed 4926.1\n",
      "Finished epoch 16, time elapsed 5040.7\n",
      "Finished epoch 17, time elapsed 5156.3\n",
      "Finished epoch 18, time elapsed 5272.0\n",
      "Finished epoch 19, time elapsed 5386.8\n",
      "Finished epoch 20, time elapsed 5500.4\n",
      "Finished epoch 21, time elapsed 5610.3\n",
      "Finished epoch 22, time elapsed 5726.6\n",
      "Finished epoch 23, time elapsed 5839.8\n",
      "Finished epoch 24, time elapsed 5956.9\n",
      "Finished epoch 25, time elapsed 6074.5\n",
      "Finished epoch 26, time elapsed 6187.9\n",
      "Finished epoch 27, time elapsed 6303.4\n",
      "Finished epoch 28, time elapsed 6408.0\n",
      "Finished epoch 29, time elapsed 6517.1\n",
      "Finished epoch 30, time elapsed 6632.1\n",
      "Finished epoch 31, time elapsed 6737.5\n",
      "Finished epoch 247, time elapsed 31399.4\n",
      "Finished epoch 248, time elapsed 31520.5\n",
      "Finished epoch 249, time elapsed 31630.8\n",
      "Finished epoch 250, time elapsed 31741.7\n",
      "Finished epoch 251, time elapsed 31857.2\n",
      "Finished epoch 252, time elapsed 31972.9\n",
      "Finished epoch 253, time elapsed 32083.7\n",
      "Finished epoch 254, time elapsed 32199.7\n",
      "Finished epoch 255, time elapsed 32319.7\n",
      "Finished epoch 256, time elapsed 32434.1\n",
      "Finished epoch 257, time elapsed 32547.4\n",
      "Finished epoch 258, time elapsed 32655.3\n",
      "Finished epoch 259, time elapsed 32770.4\n",
      "Finished epoch 260, time elapsed 32886.3\n",
      "Finished epoch 261, time elapsed 33008.2\n",
      "Finished epoch 262, time elapsed 33121.8\n",
      "Finished epoch 263, time elapsed 33235.6\n",
      "Finished epoch 264, time elapsed 33345.3\n",
      "Finished epoch 265, time elapsed 33460.7\n",
      "Finished epoch 266, time elapsed 33576.3\n",
      "Finished epoch 267, time elapsed 33691.4\n",
      "Finished epoch 268, time elapsed 33808.2\n",
      "Finished epoch 269, time elapsed 33924.0\n",
      "Finished epoch 270, time elapsed 34042.6\n",
      "Finished epoch 271, time elapsed 34162.9\n",
      "Finished epoch 272, time elapsed 34279.3\n",
      "Finished epoch 273, time elapsed 34395.7\n",
      "Finished epoch 274, time elapsed 34509.7\n",
      "Finished epoch 275, time elapsed 34625.5\n",
      "Finished epoch 276, time elapsed 34739.4\n",
      "Finished epoch 277, time elapsed 34853.7\n",
      "Finished epoch 278, time elapsed 34969.4\n",
      "Finished epoch 279, time elapsed 35081.2\n",
      "Finished epoch 353, time elapsed 43538.7\n",
      "Finished epoch 354, time elapsed 43651.9\n",
      "Finished epoch 355, time elapsed 43768.4\n",
      "Finished epoch 356, time elapsed 43881.7\n",
      "Finished epoch 357, time elapsed 44000.4\n",
      "Finished epoch 358, time elapsed 44115.5\n",
      "Finished epoch 359, time elapsed 44234.8\n",
      "Finished epoch 360, time elapsed 44350.6\n",
      "Finished epoch 361, time elapsed 44466.7\n",
      "Finished epoch 362, time elapsed 44584.2\n",
      "Finished epoch 363, time elapsed 44693.2\n",
      "Finished epoch 364, time elapsed 44808.0\n",
      "Finished epoch 365, time elapsed 44920.9\n",
      "Finished epoch 366, time elapsed 45035.6\n",
      "Finished epoch 367, time elapsed 45144.5\n",
      "Finished epoch 368, time elapsed 45262.7\n",
      "Finished epoch 369, time elapsed 45376.7\n",
      "Finished epoch 370, time elapsed 45492.6\n",
      "Finished epoch 371, time elapsed 45610.1\n",
      "Finished epoch 372, time elapsed 45723.9\n",
      "Finished epoch 373, time elapsed 45838.3\n",
      "Finished epoch 374, time elapsed 45954.0\n",
      "Finished epoch 375, time elapsed 46066.0\n",
      "Finished epoch 376, time elapsed 46182.2\n",
      "Finished epoch 377, time elapsed 46300.7\n",
      "Finished epoch 378, time elapsed 46417.1\n",
      "Finished epoch 379, time elapsed 46530.0\n",
      "Finished epoch 380, time elapsed 46645.5\n",
      "Finished epoch 381, time elapsed 46752.3\n"
     ]
    }
   ],
   "source": [
    "mb_size = 16\n",
    "assert args.n_sample % mb_size == 0\n",
    "err_sim = []\n",
    "err_combo = []\n",
    "err_feat = []\n",
    "\n",
    "arr_sim, arr_pred, arr_pred_true, arr_residue, arr_residue_true = [], [], [], [], []\n",
    "\n",
    "for idx, data in enumerate(multi_loader):\n",
    "    bx, by, bl = data\n",
    "    bx = bx.to(device).permute(1, 0, 2, 3, 4, 5)\n",
    "    # print(bx.shape)\n",
    "    with torch.no_grad():\n",
    "        # Run the simulation over multiple batches to avoid memory overflow\n",
    "        sim_bx = torch.zeros((args.n_sample, args.batch_size, 1, 64, 64)).to(device)\n",
    "        sim_feat = torch.zeros((args.n_sample, args.batch_size, args.feat_size)).to(device)\n",
    "        true_feat = torch.zeros((args.n_sample, args.batch_size, args.feat_size)).to(device)\n",
    "        for rep in range(args.n_sample // mb_size):\n",
    "            bx_in = bx[0, :, :2].to(device).permute(1, 0, 2, 3, 4)\n",
    "            # print(bx_in.shape)\n",
    "            sim_bx[rep*mb_size:(rep+1)*mb_size] = sampler.simulate(bx_in, n_sample=mb_size, n_future=args.n_future)[:, -1]\n",
    "            # print(simulated_bx.shape)\n",
    "            sim_feat[rep*mb_size:(rep+1)*mb_size] = feat_model(sim_bx[rep*mb_size:(rep+1)*mb_size].reshape(-1, 1, 64, 64)).view(\n",
    "                mb_size, args.batch_size, args.feat_size)\n",
    "            # print(feat.shape)\n",
    "            bx_in = bx[rep*mb_size:(rep+1)*mb_size, :, -1].reshape(-1, 1, 64, 64)\n",
    "            # print(bx_in.shape)\n",
    "            true_feat[rep*mb_size:(rep+1)*mb_size] = feat_model(bx_in).view(mb_size, args.batch_size, args.feat_size)\n",
    "            #    args.mbatch_size, mb_size, args.feat_size)\n",
    "            # Warning: to check that these reshapes didn't mess up ordering\n",
    "    for q_index, query in enumerate(queries_all):\n",
    "        alpha = torch.randn([1, args.batch_size, args.feat_size], requires_grad=True, device=device)\n",
    "        alpha_optim = optim.Adam([alpha], lr=args.q_learning_rate)\n",
    "\n",
    "        sim_val = query(sim_bx.reshape(-1, 1, 64, 64)).view(args.n_sample, args.batch_size)\n",
    "\n",
    "        for qiter in range(5000):\n",
    "            alpha_optim.zero_grad()\n",
    "            loss_q = (sim_val - (sim_feat * alpha).mean(dim=2)).abs().mean()\n",
    "            # print(query(bx[:, 10]))\n",
    "            # print((feat_model(bx[:, 10]) * alpha).mean(dim=1))\n",
    "            loss_q.backward()\n",
    "            alpha_optim.step()\n",
    "            writer.add_scalar('loss_q_%d' % q_index, loss_q, qiter)\n",
    "\n",
    "        # Compute the expectation if we were to use the simulated data only\n",
    "        with torch.no_grad():\n",
    "            # Expectation according to future simulations\n",
    "            sim_exp = sim_val.mean(dim=0)\n",
    "            # print(sim_exp.shape)\n",
    "            # Expectation of the features\n",
    "            # print(predictor(bx[0, :, 0:2]).shape)\n",
    "            pred_exp_feat = (predictor(bx[0, :, 0:2]) * alpha[0]).mean(dim=1)\n",
    "            residue_exp = (sim_val - (sim_feat * alpha).mean(dim=2)).mean(dim=0)\n",
    "            combo_exp = pred_exp_feat + residue_exp\n",
    "\n",
    "            true_exp = query(bx[:, :, -1].reshape(-1, 1, 64, 64)).view(args.n_sample, args.batch_size).mean(dim=0)\n",
    "            true_exp_feat = (true_feat.mean(dim=0) * alpha[0]).mean(dim=1)\n",
    "            true_exp_residue = true_exp - true_exp_feat \n",
    "            \n",
    "            err_sim.append(sim_exp - true_exp)\n",
    "            err_combo.append(combo_exp - true_exp)\n",
    "            err_feat.append(pred_exp_feat - true_exp_feat)\n",
    "            \n",
    "            arr_residue.append(residue_exp)\n",
    "            arr_pred.append(pred_exp_feat)\n",
    "            arr_sim.append(sim_exp)\n",
    "            arr_pred_true.append(true_exp_feat)\n",
    "            arr_residue_true.append(true_exp_residue)\n",
    "            \n",
    "    message(idx)\n",
    "    global_iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_sim = torch.cat(err_sim).cpu().numpy()\n",
    "err_combo = torch.cat(err_combo).cpu().numpy()\n",
    "err_feat = torch.cat(err_feat).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_residue = torch.cat(arr_residue).cpu().numpy()\n",
    "arr_pred = torch.cat(arr_pred).cpu().numpy()\n",
    "arr_pred_true = torch.cat(arr_pred_true).cpu().numpy()\n",
    "arr_sim = torch.cat(arr_sim).cpu().numpy()\n",
    "arr_residue_true = torch.cat(arr_residue_true).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05092336 0.023543902\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(err_sim)), np.mean(np.abs(err_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_combo_alt = (arr_residue_true - arr_residue) + (arr_pred_true - arr_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(err_sim, bins=20, alpha=0.5, color='b')\n",
    "plt.hist(err_combo, bins=20, alpha=0.5, color='r')\n",
    "plt.hist(err_combo_alt, bins=20, alpha=0.5, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(arr_residue.cpu().numpy(), bins=20, alpha=0.5)\n",
    "plt.hist(arr_pred, bins=20, alpha=0.5, color='r')\n",
    "plt.hist(arr_pred_true, bins=20, alpha=0.5, color='g')\n",
    "plt.hist(arr_pred - arr_pred_true, bins=20, alpha=0.5, color='b')\n",
    "\n",
    "plt.hist(arr_residue_true, bins=20, alpha=0.5, color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "for rep in range(10):\n",
    "    for t in range(12):\n",
    "        plt.subplot(20, 12, rep*24+t+1)\n",
    "        plt.imshow(bx[0, rep, t, 0].cpu())\n",
    "        plt.axis('off')\n",
    "        plt.subplot(20, 12, rep*24+12+t+1)\n",
    "        plt.imshow(simulated_bx[rep, t, 0, 0].cpu())\n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "multi_dataset.set_nsample(args.n_sample)\n",
    "plt.figure(figsize=(20, 20))\n",
    "palette = sns.color_palette('hls', 5)\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(multi_loader):\n",
    "        bx, by, bl = data\n",
    "        bx = bx.to(device)\n",
    "\n",
    "        actual_feat = feat_model(bx[:, :, -1].view(-1, 1, 64, 64)).view(8, args.n_sample, args.feat_size)\n",
    "        actual_exp = actual_feat.mean(dim=1)\n",
    "        pred_exp = predictor(bx[:, 0, 0:2])\n",
    "        \n",
    "        for i in range(36):\n",
    "            plt.subplot(6, 6, i+1)\n",
    "            plt.hist(actual_feat[0, :, i].cpu().numpy(), bins=20, color=palette[idx], alpha=0.5)\n",
    "            plt.axvline(pred_exp[0, i], color=palette[idx])\n",
    "            plt.axvline(actual_exp[0, i], color=palette[idx], linestyle=':')\n",
    "        if idx == 4:\n",
    "            break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
