{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # System configs\n",
    "        self.gpu = 3\n",
    "        self.run_label = 0\n",
    "        self.verbose = False\n",
    "        self.log_root = '/data/hdim-forecast/log5/'\n",
    "        \n",
    "        # Global model choice parameters\n",
    "        self.x_dim = 2         # Number of input features \n",
    "        self.y_dim = 131       # Number of predicted values\n",
    "        self.n_past = 32       # Number of steps from the past to condition on\n",
    "        self.n_future = 10     # Number of steps into the future to predict\n",
    "        self.feat_size = 131   # Number of basis features\n",
    "        self.lstm_hidden_dim = 256  \n",
    "        self.lstm_layers = 2\n",
    "        self.lstm_dropout = 0.5\n",
    "        self.prediction_model = 'lstm'        \n",
    "        \n",
    "        # Parameters only used for forecasting\n",
    "        self.q_learning_rate = 1e-3   # learning rate for optimizing queries\n",
    "        self.q_batch_size = 8    # batch size to evaluate each query on \n",
    "        self.n_sample = 1024   # Number of samples to simulate\n",
    "        \n",
    "        # Parameters only used for sampler training\n",
    "        self.s_test_len = 256   # The length of test sequence to visualize\n",
    "        self.s_learning_rate = 1e-4  \n",
    "        self.s_batch_size = 8\n",
    "        \n",
    "        # Parameters only used for predictor training\n",
    "        self.p_learning_rate = 1e-4\n",
    "        \n",
    "args = Args()\n",
    "device = torch.device('cuda:%d' % args.gpu)\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run number = 4\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    args.name = '%s-%d-%d-%d-%d-%d-%d-%d-%.1f-%.4f-%d' % \\\n",
    "        (args.prediction_model, args.x_dim, args.y_dim, args.n_past, args.n_future, args.feat_size, \n",
    "         args.lstm_layers, args.lstm_hidden_dim, args.lstm_dropout, args.s_learning_rate, args.run_label)\n",
    "    args.log_dir = os.path.join(args.log_root, args.name)\n",
    "    if not os.path.isdir(args.log_dir):\n",
    "        os.makedirs(args.log_dir)\n",
    "        break\n",
    "    args.run_label += 1\n",
    "print(\"Run number = %d\" % args.run_label)\n",
    "writer = SummaryWriter(args.log_dir)\n",
    "log_writer = open(os.path.join(args.log_dir, 'results.txt'), 'w')\n",
    "\n",
    "start_time = time.time()\n",
    "global_iteration = 0\n",
    "random.seed(args.run_label)  # Set a different random seed for different run labels\n",
    "torch.manual_seed(args.run_label)\n",
    "    \n",
    "def log_scalar(name, value, epoch):\n",
    "    writer.add_scalar(name, value, epoch)\n",
    "    log_writer.write('%f ' % value)\n",
    "    \n",
    "    \n",
    "def message(epoch):\n",
    "    print(\"Finished epoch %d, time elapsed %.1f\" % (epoch, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficDataset(train=True, max_len=args.n_past+args.n_future)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.s_batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = TrafficDataset(train=False, max_len=args.s_test_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Time features\n",
    "\n",
    "\n",
    "# def next_time(week, num_steps):\n",
    "#     start_time = week.item() * 3600 * 24 * 7\n",
    "#     time_new = torch.linspace(start_time, start_time + 5*60*(num_steps-1), num_steps, device=week.device)\n",
    "#     hour_of_day = (time_new % (3600 * 24)) / (3600. * 24)\n",
    "#     week_day = (time_new % (3600 * 24 * 7)) / (3600 * 24. * 7)\n",
    "#     return torch.stack([hour_of_day, week_day], dim=-1)\n",
    "\n",
    "# print(next_time(bx[0, 0, 1], 500).shape)\n",
    "# print(bx.shape)\n",
    "# bx_expanded = []\n",
    "# for i in range(args.batch_size):\n",
    "#     bx_expanded.append(torch.cat([bx[i, :32], next_time(bx[i, -1, 1], 500)]))\n",
    "# bx_expanded = torch.stack(bx_expanded)\n",
    "    \n",
    "# print(bx_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn_nll(mu: Variable, sigma: Variable, labels: Variable):\n",
    "    '''\n",
    "    Compute using gaussian the log-likehood which needs to be maximized. Ignore time steps where labels are missing.\n",
    "    Args:\n",
    "        mu: (Variable) dimension [batch_size, y_dim] - estimated mean at time step t\n",
    "        sigma: (Variable) dimension [batch_size, y_dim] - estimated standard deviation at time step t\n",
    "        labels: (Variable) dimension [batch_size, y_dim] z_t\n",
    "    Returns:\n",
    "        loss: (Variable) average log-likelihood loss across the batch\n",
    "    '''\n",
    "    mu, sigma, labels = mu.flatten(), sigma.flatten(), labels.flatten()\n",
    "    valid_index = (labels > 0)\n",
    "    distribution = torch.distributions.normal.Normal(mu[valid_index], sigma[valid_index])\n",
    "    likelihood = distribution.log_prob(labels[valid_index])\n",
    "    return -torch.mean(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMSampler(args).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.s_learning_rate)\n",
    "#scheduler = optim.lr_scheduler.StepLR(exp_optim, 20, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, time elapsed 530.2\n",
      "Finished epoch 10, time elapsed 5405.2\n",
      "Finished epoch 20, time elapsed 10247.6\n",
      "Finished epoch 30, time elapsed 15095.5\n",
      "Finished epoch 40, time elapsed 19974.8\n",
      "Finished epoch 50, time elapsed 24880.1\n",
      "Finished epoch 60, time elapsed 29782.7\n",
      "Finished epoch 70, time elapsed 34679.3\n",
      "Finished epoch 80, time elapsed 39573.9\n",
      "Finished epoch 100, time elapsed 49407.0\n",
      "Finished epoch 110, time elapsed 54290.6\n",
      "Finished epoch 120, time elapsed 59217.5\n",
      "Finished epoch 130, time elapsed 64121.8\n",
      "Finished epoch 140, time elapsed 68946.6\n",
      "Finished epoch 150, time elapsed 73666.0\n",
      "Finished epoch 160, time elapsed 78409.7\n",
      "Finished epoch 170, time elapsed 83046.7\n",
      "Finished epoch 180, time elapsed 87798.5\n",
      "Finished epoch 190, time elapsed 92535.3\n",
      "Finished epoch 200, time elapsed 97257.7\n",
      "Finished epoch 210, time elapsed 101994.8\n",
      "Finished epoch 220, time elapsed 106817.1\n",
      "Finished epoch 230, time elapsed 111544.3\n",
      "Finished epoch 240, time elapsed 116312.1\n",
      "Finished epoch 250, time elapsed 121060.1\n",
      "Finished epoch 260, time elapsed 125818.3\n",
      "Finished epoch 270, time elapsed 130574.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/shutil.py\", line 490, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/usr/lib/python3.6/shutil.py\", line 488, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-skv4n7ls'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 280, time elapsed 135317.3\n",
      "Finished epoch 290, time elapsed 140017.5\n",
      "Finished epoch 300, time elapsed 144773.5\n",
      "Finished epoch 310, time elapsed 149523.0\n",
      "Finished epoch 320, time elapsed 154267.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1000):\n",
    "    for data in train_loader:\n",
    "        bx, by = data[0].to(device), data[1].to(device)\n",
    "        # print(bx.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden, cell = net.init_hidden(bx.shape[0]), net.init_cell(bx.shape[0])\n",
    "        for t in range(args.n_past):\n",
    "            mu, sigma, hidden, cell = net(bx[:, t].unsqueeze(0), by[:, t].unsqueeze(0), hidden, cell)\n",
    "\n",
    "        loss = loss_fn_nll(mu=mu, sigma=sigma, labels=by[:, args.n_past])\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        writer.add_scalar('loss', loss, global_iteration)\n",
    "        global_iteration += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tx, ty = iter(test_loader).next()\n",
    "        tx, ty = tx.to(device).permute(1, 0, 2), ty.to(device).permute(1, 0, 2)\n",
    "    # bx_expanded = torch.cat([bx[0, :32], next_time(bx[0, -1, 1], 500)])\n",
    "        samples = net.sample(tx, ty[:args.n_past], args.s_test_len - args.n_past)\n",
    "        joined_samples = torch.cat([ty[:args.n_past], samples], dim=0)\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        for plot_id in range(16):\n",
    "            plt.subplot(8, 2, plot_id+1)\n",
    "            plt.plot(range(args.s_test_len), ty[:, 0, plot_id].cpu(), c='g', label='true')\n",
    "            plt.plot(range(args.s_test_len), joined_samples[:, 0, plot_id].cpu(), c='r', label='pred')\n",
    "            plt.ylim([0, ty[:, 0, plot_id].max().item() * 1.5])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # writer.add_figure('sample', fig, global_iteration)\n",
    "        plt.savefig(os.path.join(args.log_dir, 'result_%d.png' % (epoch // 10)))\n",
    "        plt.close()\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.cpu().state_dict(), 'pretrained/sampler_traffic_%s.pt' % args.name, _use_new_zipfile_serialization=False)\n",
    "        net.to(device)\n",
    "        message(epoch)\n",
    "        \n",
    "#         buf = io.BytesIO()\n",
    "#         plt.savefig(buf, format='jpeg')\n",
    "#         buf.seek(0)\n",
    "#         image = PIL.Image.open(buf)\n",
    "#         image = ToTensor()(image).permute(1, 2, 0)\n",
    "#         print(image.shape)\n",
    "#         plt.imshow(image)\n",
    "#         plt.show()\n",
    "#         writer.add_image(name, image, iteration)\n",
    "        \n",
    "#         write_plt(writer, 'sample', global_iteration)\n",
    "\n",
    "#         \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
